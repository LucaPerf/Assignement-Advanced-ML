{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file...\n",
      "text length 558240\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ***** first 1000 characters ***** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"inferno\\n\\n\\n\\ninferno: canto i\\n\\n\\nnel mezzo del cammin di nostra vita\\n  mi ritrovai per una selva oscura\\n  che' la diritta via era smarrita.\\n\\nahi quanto a dir qual era e` cosa dura\\n  esta selva selvaggia e aspra e forte\\n  che nel pensier rinova la paura!\\n\\ntant'e` amara che poco e` piu` morte;\\n  ma per trattar del ben ch'i' vi trovai,\\n  diro` de l'altre cose ch'i' v'ho scorte.\\n\\nio non so ben ridir com'i' v'intrai,\\n  tant'era pien di sonno a quel punto\\n  che la verace via abbandonai.\\n\\nma poi ch'i' fui al pie` d'un colle giunto,\\n  la` dove terminava quella valle\\n  che m'avea di paura il cor compunto,\\n\\nguardai in alto, e vidi le sue spalle\\n  vestite gia` de' raggi del pianeta\\n  che mena dritto altrui per ogne calle.\\n\\nallor fu la paura un poco queta\\n  che nel lago del cor m'era durata\\n  la notte ch'i' passai con tanta pieta.\\n\\ne come quei che con lena affannata\\n  uscito fuor del pelago a la riva\\n  si volge a l'acqua perigliosa e guata,\\n\\ncosi` l'animo mio, ch'ancor fuggiva,\\n  si volse a retro a r\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Opening file...')\n",
    "path = \"divina_commedia.txt\"\n",
    "with io.open(path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read().lower()\n",
    "\n",
    "print(\"text length\", len(text))\n",
    "print()\n",
    "print('\\n\\n\\n\\n\\n', '***** first 1000 characters *****', '\\n\\n\\n\\n\\n')\n",
    "text[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Canto 1:\n",
      "inferno\n",
      "--------------------------------------------------\n",
      "\n",
      "Canto 2:\n",
      "inferno\n",
      "--------------------------------------------------\n",
      "\n",
      "Canto 3:\n",
      "nel mezzo del cammin di nostra vita\n",
      "  mi ritrovai per una selva oscura\n",
      "  che' la diritta via era smarrita.\n",
      "ahi quanto a dir qual era e` cosa dura\n",
      "  esta selva selvaggia e aspra e forte\n",
      "  che nel pensier rinova la paura!\n",
      "tant'e` amara che poco e` piu` morte;\n",
      "  ma per trattar del ben ch'i' vi trovai,\n",
      "  diro` de l'altre cose ch'i' v'ho scorte.\n",
      "io non so ben ridir com'i' v'intrai,\n",
      "  tant'era pien di sonno a quel punto\n",
      "  che la verace via abbandonai.\n",
      "ma poi ch'i' fui al pie` d'un colle giunto,\n",
      "  la` \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Pulizia del testo per rimuovere righe vuote e spazi extra\n",
    "# Sostituiamo le righe vuote con uno spazio singolo e rimuoviamo gli spazi in eccesso\n",
    "text = re.sub(r'\\n+', '\\n', text)  # Rimuove sequenze multiple di linee vuote\n",
    "text = text.strip()  # Rimuove spazi extra all'inizio e alla fine\n",
    "\n",
    "# Stampa la lunghezza del testo e i primi 1000 caratteri per il controllo\n",
    "print(\"text length\", len(text))\n",
    "print()\n",
    "print('***** first 1000 characters *****')\n",
    "print(text[:1000])\n",
    "\n",
    "# Utilizzare una regex per dividere il testo in base ai canti\n",
    "# La regex cerca le intestazioni tipo \"inferno: canto i\", \"purgatorio: canto i\", \"paradiso: canto i\" \n",
    "# e divide il testo in canti separati\n",
    "\n",
    "# Regex che cerca i titoli del canto\n",
    "canti = re.split(r'(?<=inferno|purgatorio|paradiso):\\s*canto\\s+[ivxlcdm]+', text)\n",
    "\n",
    "# Rimuoviamo gli spazi extra e gli elementi vuoti dalla lista risultante\n",
    "canti = [canto.strip() for canto in canti if canto.strip()]\n",
    "\n",
    "# Verifica la separazione stampando i primi 3 canti\n",
    "for i in range(min(3, len(canti))):\n",
    "    print(f\"\\nCanto {i+1}:\")\n",
    "    print(canti[i][:500])  # Mostra i primi 500 caratteri di ciascun canto\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open(\"vita_nova.txt\", encoding='utf-8') as file:\n",
    "    external_text = file.read().lower()\n",
    "\n",
    "print(\"'vita nova' text length\", len(external_text))\n",
    "print()\n",
    "print('\\n\\n\\n\\n\\n', '***** first 1000 characters *****', '\\n\\n\\n\\n\\n')\n",
    "external_text[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text) | set(external_text)))\n",
    "print('total chars: ', len(chars))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars)) # create first dictionary\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print(char_indices)\n",
    "print(indices_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 30 # chunk length\n",
    "step = 3\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "print('number of sequences: ', len(sentences))\n",
    "print(sentences[11])\n",
    "print(next_chars[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first point of assignment\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sentences_train, sentences_val, next_chars_train, next_chars_val = train_test_split(\n",
    "    sentences, next_chars, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(sentences_train)}\")\n",
    "print(f\"Validation set: {len(sentences_val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_sentences = []\n",
    "external_next_chars = []\n",
    "\n",
    "for i in range(0, len(external_text) - maxlen, step):\n",
    "    external_sentences.append(external_text[i: i + maxlen])\n",
    "    external_next_chars.append(external_text[i + maxlen])\n",
    "\n",
    "print('number of sequences: ', len(external_sentences))\n",
    "print(external_sentences[11])\n",
    "print(external_next_chars[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode in one rapresentation\n",
    "print('generating input and output..')\n",
    "\n",
    "def one_hot_encoding(sentences, next_chars, maxlen, chars, char_indices): \n",
    "    x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Training set\n",
    "x_train, y_train = one_hot_encoding(sentences_train, next_chars_train, maxlen, chars, char_indices)\n",
    "\n",
    "# Validation set\n",
    "x_val, y_val = one_hot_encoding(sentences_val, next_chars_val, maxlen, chars, char_indices)\n",
    "\n",
    "# Test set\n",
    "x_test, y_test = one_hot_encoding(external_sentences, external_next_chars, maxlen, chars, char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "# second point\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=optimizer, \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def testAfterEpoch(epoch, _):\n",
    "    print()\n",
    "    print()\n",
    "    print('***** Epoch: %d *****' % (epoch+1))\n",
    "    start_index = random.randint(0, len(text)- maxlen - 1)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index : start_index + maxlen]\n",
    "    generated = generated + sentence\n",
    "\n",
    "    print('***** starting sentence *****') \n",
    "    print(sentence)\n",
    "    print('*****************************')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1\n",
    "        \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = np.argmax(preds)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=testAfterEpoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size = 2048, \n",
    "          epochs = 20, \n",
    "          callbacks = [print_callback],\n",
    "          validation_data =(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
