{"cells":[{"cell_type":"markdown","metadata":{"id":"IHGuzgm6295q"},"source":["# Caricamento Dataset e principali imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26281,"status":"ok","timestamp":1709136508306,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"EHncwVXM6ZJW","outputId":"e81c0d5f-a674-439e-8c4c-e455bddb947c"},"outputs":[],"source":["#from google.colab import drive\n","import numpy as np\n","import sklearn as sn\n","import pandas as pd\n","import requests\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","df = pd.read_csv(\"dataset/diabetes_data.csv\")"]},{"cell_type":"markdown","metadata":{"id":"JC9FKrbXvuYp"},"source":["# Riduzione Dataset"]},{"cell_type":"markdown","metadata":{"id":"I0nlvxyEv8ls"},"source":["Il codice riduce le dimensioni del dataset originale per migliorare i tempi di addestramento. Viene creato un nuovo dataset ridotto, chiamato `sampled_df`, che preserva la proporzione delle classi attraverso lo stratified sampling. L'analisi della distribuzione delle feature mostra visivamente che questa riduzione non influenza significativamente le caratteristiche del dataset originale, mantenendo intatte le proporzioni delle classi."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import seaborn as sns\n","\n","# Original dataset counts\n","original_counts = df['Diabetes'].value_counts()\n","\n","# Print information about the original dataset\n","print(\"Original distribution:\")\n","print(original_counts)\n","\n","# Plot the original distribution\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","bars = plt.bar(original_counts.index, original_counts.values, color=['green', 'orange'])\n","plt.title('Original distribution:')\n","plt.xlabel('Diabetes class')\n","plt.ylabel('Number of samples')\n","plt.xticks(original_counts.index, ['Not Diabetics (0.0)', 'Diabetics (1.0)'])\n","\n","# Add numeric values above the bars in the first plot\n","for bar in bars:\n","    yval = bar.get_height()\n","    plt.text(bar.get_x() + bar.get_width()/2, yval + 100, round(yval), ha='center', va='bottom', color='black')\n","\n","# Stratified sampling\n","# Split the dataset based on the 'Diabetes' variable\n","df_group_0 = df[df['Diabetes'] == 0]\n","df_group_1 = df[df['Diabetes'] == 1]\n","\n","# Specify the desired sample size for each group\n","sample_size_per_group = 20000 // 2  # Divide equally between classes\n","\n","# Perform stratified random sampling for both groups\n","sampled_group_0 = df_group_0.sample(n=sample_size_per_group, random_state=42)\n","sampled_group_1 = df_group_1.sample(n=sample_size_per_group, random_state=42)\n","\n","# Combine the samples to get the stratified dataset\n","sampled_df = pd.concat([sampled_group_0, sampled_group_1])\n","\n","# Counts in the stratified dataset\n","stratified_counts = sampled_df['Diabetes'].value_counts()\n","\n","# Print information about the stratified dataset\n","print(\"\\nStratified distribution:\")\n","print(stratified_counts)\n","\n","print(\"\\nNumber of rows original dataset:\", df.shape[0])\n","print(\"Number of rows stratified dataset:\", sampled_df.shape[0])\n","\n","# Plot the stratified distribution\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","bars = plt.bar(original_counts.index, stratified_counts.values, color=['green', 'orange'])\n","plt.title('Original distribution:')\n","plt.xlabel('Diabetes class')\n","plt.ylabel('Number of samples')\n","plt.xticks(original_counts.index, ['Not Diabetics (0.0)', 'Diabetics (1.0)'])\n","\n","# Add numeric values above the bars in the first plot\n","for bar in bars:\n","    yval = bar.get_height()\n","    plt.text(bar.get_x() + bar.get_width()/2, yval + 100, round(yval), ha='center', va='bottom', color='black')\n","\n","\n","feature_columns = [col for col in df.columns if col != 'Diabetes']\n","\n","# Plot the old and new distributions for each feature\n","for feature in feature_columns:\n","    # For BMI feature, use KDE plot\n","    if feature == 'BMI':\n","        plt.figure(figsize=(12, 6))\n","        sns.kdeplot(data=df[feature], label='Original', color='green', fill = True)\n","        sns.kdeplot(data=sampled_df[feature], label='Stratified samples', color='orange', fill = True)\n","        plt.xlabel('BMI')\n","        plt.ylabel('Density')\n","        plt.title(f'Distribution of {feature} - Original vs Stratified samples')\n","        plt.legend()\n","        plt.tight_layout()\n","        plt.show()\n","    else:\n","        # Calculate the distribution of the feature in the original dataset\n","        original_feature_counts = df[feature].value_counts(normalize=True)\n","\n","        # Calculate the distribution of the feature in the stratified sampled dataset\n","        sampled_feature_counts = sampled_df[feature].value_counts(normalize=True)\n","\n","        # Create bars for the plot\n","        categories = original_feature_counts.index.union(sampled_feature_counts.index)\n","        indices = np.arange(len(categories))\n","        bar_width = 0.35\n","\n","        # Plot the distributions\n","        plt.figure(figsize=(12, 6))\n","        plt.bar(indices, original_feature_counts.reindex(categories, fill_value=0), bar_width, label='Original', color=\"green\")\n","        plt.bar(indices + bar_width, sampled_feature_counts.reindex(categories, fill_value=0), bar_width, label='Stratified samples', color=\"orange\")\n","\n","        # Add labels, title, and legend\n","        plt.xlabel('Categories')\n","        plt.ylabel('Proportion')\n","        plt.title(f'Distribution of {feature} - Original vs Stratified samples')\n","\n","        # Rotate x-labels for BMI feature\n","        if feature == 'BMI':\n","            plt.xticks(indices[::2] + bar_width / 2, categories[::2], rotation=45)\n","        else:\n","            plt.xticks(indices + bar_width / 2, categories)\n","        plt.legend()\n","\n","        # Show the plot\n","        plt.tight_layout()\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"id":"YSA52dxMXdaS"},"source":["# Descrizione Generale Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1708824310497,"user":{"displayName":"Alessandro Cassani","userId":"00303154776563953082"},"user_tz":-60},"id":"qr2002NxXhlv","outputId":"668fb107-4125-4e39-ba3f-cc091faf9b4f"},"outputs":[],"source":["sampled_df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"executionInfo":{"elapsed":463,"status":"ok","timestamp":1708824311392,"user":{"displayName":"Alessandro Cassani","userId":"00303154776563953082"},"user_tz":-60},"id":"D0v_YKS5Y4oF","outputId":"0e55dbd2-b90e-4a03-a0f8-e71ee0449230"},"outputs":[],"source":["sampled_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1708824311392,"user":{"displayName":"Alessandro Cassani","userId":"00303154776563953082"},"user_tz":-60},"id":"1Ds8GSllZJv_","outputId":"773b7637-dae1-47b0-ee0e-644c8d593dfa"},"outputs":[],"source":["sampled_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1708824311392,"user":{"displayName":"Alessandro Cassani","userId":"00303154776563953082"},"user_tz":-60},"id":"XgRAAIyabnqR","outputId":"0c9421c4-059b-484d-aed2-f245c90aa53d"},"outputs":[],"source":["sampled_df.info()"]},{"cell_type":"markdown","metadata":{"id":"OIkWvXehb-T8"},"source":["Il dataset non presenta valori mancanti"]},{"cell_type":"markdown","metadata":{"id":"bnsv3VuebNBV"},"source":["# Prime operazioni sul dataset"]},{"cell_type":"markdown","metadata":{"id":"KOirFpIeKelm"},"source":["Conversione da float ad int , in quanto le features non presentano valori decimali"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H61wJ86ebQSu"},"outputs":[],"source":["sampled_df = sampled_df.astype(int)"]},{"cell_type":"markdown","metadata":{"id":"q18qvQyVcTNJ"},"source":["Controllo presenza di valori nulli"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1709136536185,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"YNxbotqUcaGy","outputId":"09df06fe-ee67-494d-fe6f-cbbe7028ef47"},"outputs":[],"source":["sampled_df.isnull().sum().any()"]},{"cell_type":"markdown","metadata":{"id":"bUev54w-ciew"},"source":["Controllo presenza valori duplicati"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1709136536186,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"yodCKqoUclea","outputId":"5f172573-4131-41b9-8aaf-5c661e6b9c0f"},"outputs":[],"source":["sampled_df.duplicated().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ynkn3_mXxLhs"},"outputs":[],"source":["sampled_df = sampled_df.drop_duplicates()"]},{"cell_type":"markdown","metadata":{"id":"WrPt54ABS2IP"},"source":["Rinomina della colonna Gender in Sex"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1709136536186,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"71Q23gc-S0_T","outputId":"782acb33-5743-4bc4-b2f2-814e7af3a520"},"outputs":[],"source":["sampled_df.rename(columns={'Sex': 'Gender'}, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"yCZVROPoH3x8"},"source":["# Analisi statistica univariata"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":420},"executionInfo":{"elapsed":7902,"status":"ok","timestamp":1708747291822,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"3eIE-naKH61A","outputId":"ce41bb63-c073-4577-8d6d-210be2242a5a"},"outputs":[],"source":["from matplotlib.ticker import MultipleLocator\n","\n","def printPlots():\n","    total_rows = len(sampled_df)\n","    for column in sampled_df.columns:\n","        if column == 'Age':\n","            fig, ax = plt.subplots(figsize=(10, 7))\n","            bins = np.arange(sampled_df[column].min(), sampled_df[column].max() + 2)\n","            hist, bins, _ = ax.hist(sampled_df[column], bins=bins, edgecolor='black', align='left', color='green')\n","            ax.set_xlabel('Age')\n","            ax.set_ylabel('Frequency')\n","            ax.set_title(f'Bar Plot of {column}')\n","            ax.set_xticks(bins)\n","            plt.show()\n","        elif column == 'BMI':\n","            fig, ax = plt.subplots(figsize=(10, 6))\n","            ax.boxplot(df[column])\n","            ax.set_xlabel('BMI')\n","            ax.set_ylabel('Values')\n","            ax.set_title(f'Boxplot of {column}')\n","            ax.yaxis.set_major_locator(MultipleLocator(10))\n","            plt.show()\n","            printScatterPlot('BMI')\n","        elif column in ['GenHlth', 'MentHlth', 'PhysHlth']:\n","            fig, ax = plt.subplots(figsize=(10, 7))\n","            if column == 'GenHlth':\n","              ax.set_title(f'Generic Health (scale 1-5)')\n","              ax.set_xlabel('generic health')\n","              bins = np.arange(sampled_df[column].min(), sampled_df[column].max() + 2)\n","              hist, bins, _ = ax.hist(sampled_df[column], bins=bins, edgecolor='black', align='left', color='green')\n","              ax.set_xticks(bins)\n","            elif column == 'PhysHlth':\n","              ax.set_title(f'physical illness or injury days in past 30 days scale 1-30')\n","              ax.set_xlabel('physical health')\n","              bins = np.arange(sampled_df[column].min(), sampled_df[column].max() + 2)\n","              hist, bins, _ = ax.hist(sampled_df[column], bins=bins, edgecolor='black', align='left', color='green')\n","              ax.set_xticks(bins)\n","              printScatterPlot('PhysHlth')\n","            elif column == 'MentHlth':\n","              ax.set_title(f'days of poor mental health scale 1-30 days')\n","              ax.set_xlabel('mental health')\n","              bins = np.arange(sampled_df[column].min(), sampled_df[column].max() + 2)\n","              hist, bins, _ = ax.hist(sampled_df[column], bins=bins, edgecolor='black', align='left', color='green')\n","              ax.set_xticks(bins)\n","              printScatterPlot('MentHlth')\n","\n","            ax.set_ylabel('Frequency')\n","            plt.show()\n","\n","        elif column == 'Diabetes':\n","            counts = sampled_df[column].value_counts()\n","            percentages = [count / total_rows * 100 for count in counts]\n","            plt.pie(percentages, autopct='%1.1f%%', colors=['green', 'orange'])\n","            plt.title(f'Proportion of {column} (target)')\n","            labels = ['yes', 'no']\n","            plt.legend(labels=labels, loc='upper right')\n","            plt.show()\n","        else:\n","            fig, ax = plt.subplots(figsize=(8, 6))\n","            column_counts = sampled_df[column].value_counts()\n","            percentages = [count / total_rows * 100 for count in column_counts]\n","            ax.set_xlabel(f'{column} feature')\n","            ax.set_ylabel('Percentage')\n","            legend_labels = ['no', 'yes']\n","            if column == 'Gender':\n","                ax.set_title(f'Gender')\n","                legend_labels = ['Female', 'Male']\n","            elif column == 'HighChol':\n","                ax.set_title(f'High cholesterol values')\n","            elif column == 'CholCheck':\n","                ax.set_title(f'Cholesterol check in the past 5 years')\n","                percentages.reverse()\n","                legend_labels = ['no', 'yes']\n","            elif column == 'Smoker':\n","                ax.set_title(f'smoked more than 100 cigarettes in entire life')\n","            elif column == 'HeartDiseaseorAttack':\n","                ax.set_title(f'coronary heart disease (CHD) or myocardial infarction (MI)')\n","            elif column == 'PhysActivity':\n","                ax.set_title(f'physical activity in past 30 days')\n","                percentages.reverse()\n","                legend_labels = ['no', 'yes']\n","            elif column == 'Fruits':\n","                ax.set_title(f'Consume Fruit 1 or more times per day')\n","                percentages.reverse()\n","                legend_labels = ['no', 'yes']\n","            elif column == 'Veggies':\n","                ax.set_title(f'Consume Vegetables 1 or more times per day')\n","                percentages.reverse()\n","                legend_labels = ['no', 'yes']\n","            elif column == 'HvyAlcoholConsump':\n","                ax.set_title(f'Heavy alcohol consume')\n","            elif column == 'DiffWalk':\n","                ax.set_title(f'serious difficulty walking or climbing stairs')\n","            elif column == 'Stroke':\n","                ax.set_title(f'ever had a stroke')\n","            elif column == 'HighBP':\n","                ax.set_title(f'High blood pressure')\n","                percentages.reverse()\n","                legend_labels = ['no', 'yes']\n","\n","            bars = ax.bar(['0', '1'], percentages, color=['green', 'orange'])\n","            for bar in bars:\n","                height = bar.get_height()\n","                ax.text(bar.get_x() + bar.get_width()/2, height, f'{height:.2f}%', ha='center', va='bottom')\n","            if 'legend_labels' in locals():\n","                ax.legend(handles=[plt.Rectangle((0,0),1,1, color='green'),\n","                                   plt.Rectangle((0,0),1,1, color='orange')],\n","                          labels=legend_labels)\n","            plt.show()\n","\n","def printScatterPlot(label):\n","    x_values = np.random.uniform(0.5, 1.5, size=len(sampled_df[label]))\n","    plt.figure(figsize=(8, 6), facecolor='white')\n","    plt.scatter(x_values, sampled_df[label], s=10, color='green', alpha=0.5)\n","    plt.title(f'scatter plot of {label}')\n","    plt.ylabel(label)\n","    plt.show()\n","\n","printPlots()"]},{"cell_type":"markdown","metadata":{"id":"asmYiy7OBrZ8"},"source":["# Analisi Statistica Multivariata"]},{"cell_type":"markdown","metadata":{"id":"5hYU-T9RvncX"},"source":["\n"," Frequenza Diabete per features categoriche"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":555},"executionInfo":{"elapsed":6277,"status":"ok","timestamp":1708747300913,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"c3PQy82mBye0","outputId":"d95bcc5d-6612-4559-cf31-877ec555d0e4"},"outputs":[],"source":["from matplotlib.ticker import MaxNLocator\n","\n","features = [x for x in sampled_df.columns if x not in ['BMI', 'PhysHlth', 'Diabetes', 'MentHlth']]\n","\n","def plot_distribution_by_diabetes(sampled_df, columns):\n","    plt.figure(figsize=(14, len(columns)*4))\n","    for i, column in enumerate(features):\n","        plt.subplot(len(columns) // 2 + 1, 2, i + 1)\n","        palette = ['green','orange']\n","        unique_values = sampled_df[column].unique()\n","        ax = sns.countplot(x=column, hue=\"Diabetes\", data=sampled_df, palette=palette)\n","        plt.title(f'Distribution of {column} compared to Diabetes feature')\n","        plt.xlabel('')\n","\n","        if column == 'GenHlth':\n","            # Set x-label ticks for GenHlth to the values from 1 to 5\n","            ax.set_xticklabels(['1', '2', '3', '4', '5'])\n","        else:\n","          if column == 'Gender':\n","              plt.xticks([0, 1], [f'female', f'male'])\n","          else:\n","            plt.xticks([0, 1], [f'No {column}', f'Yes {column}'])\n","\n","        # Change legend labels for Diabetes feature\n","        handles, labels = ax.get_legend_handles_labels()\n","        ax.legend(handles=handles, labels=['No', 'Yes'], title='Diabetes')\n","\n","    plt.tight_layout()\n","\n","columns = sampled_df.columns.drop(\"Diabetes\")  # Remove \"Diabetes\" from the list of columns\n","plot_distribution_by_diabetes(sampled_df, columns)\n","plt.show()\n","\n","binaryFeatures = [x for x in sampled_df.columns if x not in ['BMI', 'PhysHlth', 'Diabetes', 'MentHlth','Age','GenHlth']]\n","\n","for column in binaryFeatures:\n","  countDiabetes = len(sampled_df[(sampled_df[column] == 1) & (sampled_df['Diabetes'] == 1)])\n","  countNotDiabetes = len(sampled_df[(sampled_df[column] == 1) & (sampled_df['Diabetes'] == 0)])\n","\n","  totalCounts = countDiabetes + countNotDiabetes\n","  result = countDiabetes/totalCounts*100\n","\n","  print(f'{column} == 1 diabetes rate of {result}')\n","\n","print('\\n')\n","for column in binaryFeatures:\n","  countDiabetes = len(sampled_df[(sampled_df[column] == 0) & (sampled_df['Diabetes'] == 1)])\n","  countNotDiabetes = len(sampled_df[(sampled_df[column] == 0) & (sampled_df['Diabetes'] == 0)])\n","\n","  totalCounts = countDiabetes + countNotDiabetes\n","  result = countDiabetes/totalCounts*100\n","\n","  print(f'{column} == 0 diabetes rate of {result}')\n"]},{"cell_type":"markdown","metadata":{"id":"YVW8HgatKDdJ"},"source":["Frequenza Diabete per features non categoriche"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":1952,"status":"ok","timestamp":1708747302851,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"chNqGVq4KHmC","outputId":"3136463b-c5cb-41fb-ba8f-6a36636415ae"},"outputs":[],"source":["import seaborn as sb\n","\n","features = ['BMI', 'MentHlth', 'PhysHlth']\n","\n","def plot_distribution_by_diabetes(data, features):\n","    fig, ax = plt.subplots(3, 2, figsize=(15, 15))\n","    k = 0\n","    palette = ['green', 'orange']\n","    for i in range(3):\n","        for j in range(2):\n","            if k < len(features):\n","                col = features[k]\n","                if col in ['BMI', 'MentHlth', 'PhysHlth']:\n","                    sb.kdeplot(data=data, x=col, hue='Diabetes', ax=ax[i, j], palette=palette, legend=True, fill=True)\n","                else:\n","                    sb.countplot(data=data, x=col, hue='Diabetes', ax=ax[i, j], palette=palette, legend=True, fill=True)\n","                ax[i, j].grid(False)\n","            else:\n","                ax[i, j].axis('off')\n","            k += 1\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_distribution_by_diabetes(sampled_df, features)\n"]},{"cell_type":"markdown","metadata":{"id":"aRbxY-9Qui-T"},"source":["Correlazione tra ogni feature e il target Diabete"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1708747303208,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"5f_Z_uZhuiQU","outputId":"a1f429d4-0ef9-430f-f701-b1feea4aee4a"},"outputs":[],"source":["sampled_df.drop('Diabetes', axis=1).corrwith(df.Diabetes).plot(kind='bar', grid=True, figsize=(10, 6), title=\"Correlation with Diabetes\",color=\"orange\");"]},{"cell_type":"markdown","metadata":{"id":"FP9b908nv1Ki"},"source":["Correlazione tra qualsiasi features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"elapsed":2125,"status":"ok","timestamp":1708747305326,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"cIJWJ9kOv4SK","outputId":"2f92510f-b421-47d0-a1e7-f0e1914c6435"},"outputs":[],"source":["sns.set(rc = {'figure.figsize':(16,11)})\n","sns.heatmap(sampled_df.corr(),vmin=-1, vmax=1, annot = True, fmt='.1g',cmap= 'coolwarm')"]},{"cell_type":"markdown","metadata":{"id":"AODszHhsl4OW"},"source":["# Eliminazione degli outlier e selezione delle features"]},{"cell_type":"markdown","metadata":{"id":"GNbcTGOwmLbx"},"source":["Eliminazione dei valori di BMI superiori a 60. Come dimostrato in precedenza, per valori superiori a 60 c'è una perdita di contenuto informativo utile per la predizione del diabete"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3ywJC5Zl_ye"},"outputs":[],"source":["sampled_df = sampled_df.drop(sampled_df[sampled_df['BMI'] > 60].index)"]},{"cell_type":"markdown","metadata":{"id":"vp-FpBdsnoub"},"source":["Eliminazione valori di mental health con count rate inferiore a 80"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dyDdMEthnzkN"},"outputs":[],"source":["df_count = sampled_df['MentHlth'].value_counts()\n","\n","values_to_drop = df_count[df_count < 80].index\n","\n","sampled_df = sampled_df.drop(sampled_df[sampled_df['MentHlth'].isin(values_to_drop)].index)"]},{"cell_type":"markdown","metadata":{"id":"fZOyDqGLoNb4"},"source":["Eliminazione dei valori di physic health con count rate inferiore a 80"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xgccG0jWogkE"},"outputs":[],"source":["sampled_df_count = sampled_df['PhysHlth'].value_counts()\n","\n","values_to_drop = sampled_df_count[sampled_df_count < 80].index\n","\n","sampled_df = sampled_df.drop(sampled_df[sampled_df['PhysHlth'].isin(values_to_drop)].index)"]},{"cell_type":"markdown","metadata":{"id":"rqicS3MApr8w"},"source":["Come mostrato negli ultimi grafici relativi alla correlazione tra features e target, è stato deciso di eliminare le features con meno contenuto informativo e una correlazione tra target inferiore/maggiore di +-0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AnGoyo5lqEos"},"outputs":[],"source":["sampled_df.drop(columns=['Gender'], inplace=True)\n","sampled_df.drop(columns=['Smoker'], inplace=True)\n","sampled_df.drop(columns=['Fruits'], inplace=True)\n","sampled_df.drop(columns=['Veggies'], inplace=True)\n","sampled_df.drop(columns=['MentHlth'], inplace=True)\n","sampled_df.drop(columns=['HvyAlcoholConsump'], inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"qAe66uneDoqD"},"source":["# Albero Decisionale"]},{"cell_type":"markdown","metadata":{"id":"5M3Eijtn_2sr"},"source":["Questo segmento di codice illustra il passo essenziale di suddivisione di un dataset in set di addestramento e di test, una pratica fondamentale nell'apprendimento automatico. Viene applicato un rapporto di divisione del 70-30, con il 70% dei dati utilizzato per addestrare il modello e il 30% riservato per valutarne le prestazioni. Il parametro random_state è impostato su 42 per garantire la riproducibilità della divisione.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DspHuHxuF0Gu"},"outputs":[],"source":["#Suddivdiamo il dataset in training set e test set (con test size del 30%)\n","from sklearn.model_selection import train_test_split\n","X = sampled_df.drop('Diabetes', axis=1)\n","y = sampled_df['Diabetes']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"]},{"cell_type":"markdown","metadata":{"id":"nw9htehr5BZS"},"source":["Questo codice utilizza un classificatore basato su albero decisionale per addestrare un modello per la predizione della presenza diabete.Le performance del modello vengono stampate attraverso i report di classificazione sia per il set di addestramento che per quello di test."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1709136547160,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"3KsXXS4Nue25","outputId":"e8fd2e18-f6e5-43c7-d254-71e0fc204cb0"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import classification_report\n","\n","# Creazione del classificatore dell'albero decisionale\n","model_dt = DecisionTreeClassifier(random_state=42)\n","\n","# Addestramento del modello sul set di addestramento\n","model_dt.fit(X_train, y_train)\n","\n","# Predizione sul set di addestramento e test\n","pred_train = model_dt.predict(X_train)\n","pred_test = model_dt.predict(X_test)\n","\n","# Valutazione delle prestazioni sul set di addestramento\n","print(\"Prestazioni sul Set di Addestramento:\")\n","print(classification_report(y_train, pred_train))\n","\n","# Valutazione delle prestazioni sul set di test\n","print(\"\\nPrestazioni sul Set di Test:\")\n","print(classification_report(y_test, pred_test))\n"]},{"cell_type":"markdown","metadata":{"id":"v4ppda2C6E9r"},"source":["Questo codice calcola e visualizza la curva ROC, oltre a calcolare l'AUC (Area Under the Curve) per un modello di classificazione binaria. La curva ROC e l'AUC valutano la capacità del modello di distinguere tra le classi positive e negative, fornendo un riassunto conciso delle sue prestazioni."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"executionInfo":{"elapsed":443,"status":"ok","timestamp":1709136549612,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"kooGunTZSg8E","outputId":"394eb6ae-1697-47a3-977a-b96799e8a43d"},"outputs":[],"source":["from sklearn.metrics import roc_curve, roc_auc_score\n","import matplotlib.pyplot as plt\n","# Calcolo delle probabilità predette per la classe positiva\n","y_pred_prob = model_dt.predict_proba(X_test)[:, 1]\n","\n","# Calcolo delle metriche della curva ROC\n","fpr, tpr, thresholds = roc_curve(y_test, pred_test)\n","\n","# Calcolo dell'Area Under the Curve (AUC) della curva ROC\n","roc_auc = roc_auc_score(y_test, y_pred_prob)\n","\n","# Plot della curva ROC utilizzando Matplotlib\n","plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n","\n","# Visualizzazione dell'AUC Score\n","print(\"AUC Score:\", roc_auc)\n"]},{"cell_type":"markdown","metadata":{"id":"YA7SMR1Yk26G"},"source":["La visualizzazione della matrice di confusione offre una rappresentazione chiara e sintetica delle prestazioni di un modello di classificazione."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":935},"executionInfo":{"elapsed":663,"status":"ok","timestamp":1708746108127,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"HTuftIbaF8-q","outputId":"9e0415e2-2079-413c-fd4f-b6b3140ffb0d"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","cm = confusion_matrix(y_test,pred_test)\n","labels = [1,0]\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","disp.plot();"]},{"cell_type":"markdown","metadata":{"id":"8-lp8Wov7p-4"},"source":["Nonostante i risultati complessivi siano positivi, è evidente una disparità significativa tra le prestazioni nel training set, che sono estremamente confortanti, e quelle nel test set. Tale discrepanza suggerisce la presenza di overfitting nel modello, indicando una sovraadattamento eccessivo ai dati di addestramento. Al fine di affrontare questa problematica, si opta per l'utilizzo di RandomSearch, una procedura che consente di individuare gli iperparametri ottimali. L'obiettivo è massimizzare il valore dell'Area Under the Receiver Operating Characteristic curve (AUC) e, allo stesso tempo, mitigare l'overfitting, cercando una configurazione degli iperparametri che consenta una migliore generalizzazione del modello a nuovi dati."]},{"cell_type":"markdown","metadata":{"id":"L5Ia0s6m9zNi"},"source":["RandomizedSearch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13636,"status":"ok","timestamp":1709136583977,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"CFg_8ZvJ1iU8","outputId":"1c465612-a9e1-454f-8b36-a2e6a5a151da"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.metrics import classification_report, roc_auc_score\n","from time import time\n","import numpy as np\n","\n","# Creazione del classificatore dell'albero decisionale\n","tree_classifier = DecisionTreeClassifier(random_state=42)\n","\n","# Definizione della griglia degli iperparametri\n","param_dist = {\n","    'criterion': ['gini', 'entropy'],\n","    'max_features': ['sqrt', 'log2', None],\n","    'min_samples_split': np.arange(2, 51, 2),\n","    'min_samples_leaf': np.arange(1, 9),\n","    'max_depth': [None, 5, 10, 15, 20]\n","}\n","\n","# Utilizzo di RandomizedSearchCV per una ricerca efficiente degli iperparametri\n","random_search = RandomizedSearchCV(tree_classifier, param_distributions=param_dist, n_iter=100, cv=10, scoring='roc_auc', random_state=42)\n","\n","# Misurazione del tempo di inizio per la ricerca degli iperparametri\n","start_time_hyperparameter_search = time()\n","random_search.fit(X_train, y_train)\n","end_time_hyperparameter_search = time()\n","hyperparameter_search_time = end_time_hyperparameter_search - start_time_hyperparameter_search\n","\n","# Ottenimento dei migliori parametri e del miglior stimatore\n","best_params = random_search.best_params_\n","best_tree_classifier = random_search.best_estimator_\n","\n","# Stampa dei migliori parametri e del loro ROC AUC score sul set di addestramento\n","y_train_pred_prob = best_tree_classifier.predict_proba(X_train)[:, 1]\n","roc_auc_train = roc_auc_score(y_train, y_train_pred_prob)\n","\n","# Addestramento del modello sull'intero set di addestramento e misurazione del tempo di addestramento\n","start_time_training = time()\n","best_tree_classifier.fit(X_train, y_train)\n","end_time_training = time()\n","dt_training_time = end_time_training - start_time_training\n","\n","# Previsioni sui set di addestramento e test\n","y_train_pred = best_tree_classifier.predict(X_train)\n","y_test_pred = best_tree_classifier.predict(X_test)\n","\n","# Stampa delle prestazioni sul set di addestramento\n","print(\"\\nPrestazioni sul Set di Addestramento:\")\n","print(classification_report(y_train, y_train_pred))\n","\n","# Stampa delle prestazioni sul set di test\n","print(\"\\nPrestazioni sul Set di Test:\")\n","print(classification_report(y_test, y_test_pred))\n","\n","# Stampa dei migliori parametri e del tempo impiegato per la ricerca degli iperparametri e l'addestramento\n","print(\"\\nMigliori Parametri:\", best_params)\n","print(\"Tempo impiegato per la Ricerca degli Iperparametri:\", hyperparameter_search_time, \"secondi\")\n","print(\"Tempo impiegato per l'Addestramento:\", dt_training_time, \"secondi\")\n"]},{"cell_type":"markdown","metadata":{"id":"mgYmU-AZ-7iL"},"source":["Confusion Matrix , dopo la ricerca degli iperparametri"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":940},"executionInfo":{"elapsed":663,"status":"ok","timestamp":1708746138817,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"8uJsiwUoTI0f","outputId":"48fc5fee-9a35-4a76-b78a-392f73b3ebae"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","cm = confusion_matrix(y_test,y_test_pred)\n","labels = [1,0]\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","disp.plot();"]},{"cell_type":"markdown","metadata":{"id":"fY1ICXV6_JXN"},"source":["### Curva ROC per albero di decisione"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":976},"executionInfo":{"elapsed":876,"status":"ok","timestamp":1708746144534,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"aqCVtOJ5S38h","outputId":"13adb2f7-334e-40cd-bb94-c246b2c7007c"},"outputs":[],"source":["from sklearn.metrics import roc_curve, roc_auc_score\n","import matplotlib.pyplot as plt\n","\n","y_pred_prob = random_search.predict_proba(X_test)[:, 1]\n","\n","fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n","\n","roc_auc = roc_auc_score(y_test, y_pred_prob)\n","\n","plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n","\n","print(\"AUC Score:\", roc_auc)\n"]},{"cell_type":"markdown","metadata":{"id":"0NLeJQgR9U_e"},"source":["Dopo l'implementazione della ricerca di iperparametri, si osserva un miglioramento significativo nelle prestazioni del modello. La strategia di Randomized Search ha portato a una configurazione ottimizzata degli iperparametri\n","Complessivamente, questo indicativo di un modello ottimizzato con migliori prestazioni rispetto alla configurazione iniziale."]},{"cell_type":"markdown","metadata":{"id":"GiNrdTWZ_NnE"},"source":["### 10-Fold Cross Validation"]},{"cell_type":"markdown","metadata":{"id":"KIESAiLW-UoG"},"source":["Questo frammento di codice implementa la validazione incrociata con K-Fold, una strategia fondamentale per valutare le prestazioni del modello su dati diversi. La classe KFold viene utilizzata per suddividere il set di addestramento in 10 fold, consentendo iterazioni attraverso diverse combinazioni di dati di addestramento e di validazione.\n","\n","L'utilizzo della validazione incrociata K-Fold è essenziale in quanto fornisce una stima robusta delle prestazioni del modello su diverse suddivisioni dei dati di addestramento, riducendo il rischio di valutazioni distorte dovute a particolari configurazioni casuali dei dati."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gW2SzZiQ8_yx"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score\n","\n","n_fold = 10\n","folds = KFold(n_splits=n_fold, shuffle=True)\n","\n","accuracy_k_fold_dt = []\n","\n","for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train, y_train)):\n","    X_train_fold, X_valid_fold = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n","    y_train_fold, y_valid_fold = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n","\n","    random_search.fit(X_train_fold, y_train_fold)\n","    # Fai previsioni sul set di validazione e calcola l'accuratezza\n","    y_valid_pred = random_search.predict(X_valid_fold)\n","    accuracy_k_fold_dt.append(accuracy_score(y_valid_fold, y_valid_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":254,"status":"ok","timestamp":1708746597778,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"u6xdy5Hp9B0N","outputId":"22191cc3-674f-4b3e-e26a-b5e3e209ef79"},"outputs":[],"source":["accuracy_k_fold_dt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1708746602162,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"UNZvfGlq9D8a","outputId":"16cc653f-793c-422c-94b5-76cd99e3afd3"},"outputs":[],"source":["import numpy as np\n","import scipy.stats as st\n","confidence_interval_dt = st.t.interval(confidence=0.95, df=len(accuracy_k_fold_dt)-1, loc=np.mean(accuracy_k_fold_dt), scale=st.sem(accuracy_k_fold_dt))\n","confidence_interval_dt"]},{"cell_type":"markdown","metadata":{"id":"QMY4NrXC_Lgd"},"source":["La validazione incrociata con K-Fold ha ulteriormente supportato queste osservazioni, con un intervallo di confidenza del 95%. Questo suggerisce una buona capacità di generalizzazione su diverse partizioni del dataset di addestramento."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":958},"executionInfo":{"elapsed":837,"status":"ok","timestamp":1708746613562,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"msRjfdBa-dZO","outputId":"4d325886-758a-4cb7-c5f7-56815190dead"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","# Calculate mean and confidence interval on k-fold\n","mean_accuracy_dt = np.mean(accuracy_k_fold_dt)\n","# Plot the mean and confidence interval\n","plt.errorbar(1, mean_accuracy_dt, yerr=(confidence_interval_dt[1] - confidence_interval_dt[0])/2, fmt='o', label='Decision Tree')\n","# Add labels and title\n","plt.xlabel('Group')\n","plt.ylabel('Value')\n","plt.title('Mean with Confidence Interval')\n","# Show the plot\n","plt.legend()\n","plt.show()\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5JA55SJu_coe"},"source":["La visualizzazione dell'albero di decisione ottenuto dalla fase di addestramento con la ricerca di iperparametri fornisce una rappresentazione chiara e intuitiva delle decisioni che il modello prende per classificare le istanze. Ogni nodo dell'albero rappresenta una condizione su una feature, con le linee di decisione che si biforcano a seconda dell'esito della condizione."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":949},"executionInfo":{"elapsed":4761,"status":"error","timestamp":1709136802175,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"-DYMHlNx5G1_","outputId":"dd8c142f-253c-41d9-a53c-b67fd4e4c7a5"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier, plot_tree\n","import matplotlib.pyplot as plt\n","\n","# Assuming random_search.best_estimator_ is your DecisionTreeClassifier and X_train is your training data\n","\n","plt.figure(figsize=(20, 10))\n","#tree_plot = plot_tree(random_search.best_estimator_, filled=True, feature_names=X_train.columns, class_names=['0', '1'], rounded=True, impurity=False, fontsize=8, impurity=False, fontsize=8)\n","\n","# Get the labels of each node and display them\n","#text = tree_plot[0].to_text()\n","#print(\"Node Labels:\\n\", text)\n","\n","plot_tree(random_search.best_estimator_, filled=True, feature_names=X_train.columns, class_names=['0', '1'], rounded=True, impurity=False, fontsize=8)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"91kMcG-sKUq9"},"source":["# Reti Neurali"]},{"cell_type":"markdown","metadata":{"id":"KXfpIK1fzPRT"},"source":["In questa sezione viene presentato il modello inerente alle reti neurali"]},{"cell_type":"markdown","metadata":{"id":"dvZFR6HRzP7D"},"source":["Come prima operazione si effettua la suddivisione del dataset in training set e test set. Viene applicato un rapporto di divisione del 70-30, con il 70% dei dati utilizzato per addestrare il modello e il 30% riservato per valutarne le prestazioni. Il parametro random_state è impostato su 42 per garantire la riproducibilità della divisione."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8gCofibKDu_"},"outputs":[],"source":["# Estrazione delle features e variabile target\n","y = sampled_df['Diabetes']\n","X = sampled_df.drop(columns=['Diabetes'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"khzqGJPDKX3g"},"outputs":[],"source":["# Suddivisione dataset in training set e test set (con dimensione del test_size del 30%)\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"GOxn4I9vzT2J"},"source":["In questo blocco di codice si definisce l'architettua della rete neurale. Il layer di input è composto da 11 neuroni, corrispondenti alle 11 features di input presenti nei dati. Il secondo layer ha 11 neuroni, mentre il layer di output possiede un solo neurone. Essendo in un problema di classificazione binaria, le funzioni di attivazioni scelte sono ReLU e sigmoid, mentre per la funzione di perdita (loss), si utilizza binary_crossentropy."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1708747316807,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"B_oGYi6vNQBY","outputId":"2cd2ec56-d6db-4be3-9d82-5ca4082c56a3"},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense\n","\n","# Definiamo l'architettura della rete neurale:\n","# Layer di input con 11 neuroni (perché ci sono 11 features di input)\n","# Layer nascosto con 11 neuroni\n","# Layer di output con 1 neurone, poiché l'output è 0 o 1\n","model = Sequential()\n","model.add(Dense(11, input_shape=(11,), activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compilazione del modello\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","keras.utils.plot_model(model, show_shapes=True)"]},{"cell_type":"markdown","metadata":{"id":"8oBfAE0BzW_z"},"source":["Questa sezione di codice addestra la rete neurale con 40 epoche e un batch size di 10. Viene stampato il tempo totale di addestramento, valutato le prestazioni del modello sul set di test e si visualizzano i grafici loss e dell'accuratezza durante l'addestramento."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":150136,"status":"ok","timestamp":1708747466932,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"oPP2fmyUNVPd","outputId":"34f87760-6cf9-417d-afc6-ba5d712a4fbe"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import time\n","\n","# Salva il tempo di inzio dell'addestramento\n","start_time = time.time()\n","\n","# Addestramento del modello\n","history = model.fit(X_train, y_train, epochs=40, batch_size=10, verbose=1, validation_data=(X_test, y_test))\n","\n","# Salva il tempo di fine dell'addestramento e calcola la durata totale\n","end_time = time.time()\n","nn_training_time = end_time - start_time\n","\n","# Stampa il tempo totale di addestramento\n","print(f\"Training time: {nn_training_time} secondi\")\n","\n","# Valuta il modello sul set di test\n","score = model.evaluate(X_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])\n","\n","# Stampa del grafico di training e validation loss\n","train_loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(train_loss) + 1)\n","\n","plt.plot(epochs, train_loss, label='Training Loss')\n","plt.plot(epochs, val_loss, label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","# Estrazione dell'accuratezza di training e di validation\n","train_accuracy = history.history['accuracy']\n","val_accuracy = history.history['val_accuracy']\n","\n","epochs = range(1, len(train_accuracy) + 1)\n","\n","# Stampa il grafico del training e validation accuracy\n","plt.plot(epochs, train_accuracy, 'b-', label='Training Accuracy')\n","plt.plot(epochs, val_accuracy, 'r--', label='Validation Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"GoigiElRzgxw"},"source":["In questa sezione di codice vengono stampate le performance del modello attraverso i report di classificazione sia per il set di addestramento che per il set di test."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1838,"status":"ok","timestamp":1708747468754,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"W2WUph3RI95Z","outputId":"8b351274-99f3-4d39-f8c4-ecb91159dfbc"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","import numpy as np\n","\n","# Ottenere le previsioni per il set di test\n","y_pred_test = model.predict(X_test)\n","\n","# Arrotondare le previsioni per ottenere una previsione binaria\n","y_pred_test_bin = np.round(y_pred_test)\n","\n","# Ottenere le previsioni per il set di train\n","y_pred_train = model.predict(X_train)\n","\n","# Arrotondare le previsioni per ottenere una previsione binaria\n","y_pred_train_bin = np.round(y_pred_train)\n","\n","print(\"Prestazioni sul Set di Addestramento:\")\n","print(classification_report(y_train, y_pred_train_bin))\n","\n","print(\"Prestazioni sul Set di Test:\")\n","print(classification_report(y_test, y_pred_test_bin))"]},{"cell_type":"markdown","metadata":{"id":"mTFisYeXzjh7"},"source":["In questo blocco di codice viene calcolata la matrice di confusione"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":935},"executionInfo":{"elapsed":852,"status":"ok","timestamp":1708747469597,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"yVcImU8MIyig","outputId":"5fabd0ca-c71b-405a-cc93-5a4eca625b83"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","# Calcolo della matrice di confusione\n","cm = confusion_matrix(y_pred_test_bin, y_test)\n","labels = [1,0]\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","disp.plot();"]},{"cell_type":"markdown","metadata":{"id":"-cxoNdAcznV-"},"source":["In questo blocco di codice viene calcolata la curva ROC e l'Area Under Curve (AUC)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":727},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1708747469598,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"DaHRwc5GJefk","outputId":"683e045d-d169-40a9-fc60-0ff2ba651aad"},"outputs":[],"source":["\n","from sklearn.metrics import roc_curve, roc_auc_score\n","\n","# Calcolo della curva ROC\n","fpr, tpr, thresholds = roc_curve(y_test, y_pred_test_bin)\n","\n","# Calcolo Area Under the Curve (AUC)\n","roc_auc = roc_auc_score(y_test, y_pred_test_bin)\n","\n","# Stampa curva ROC\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)'% roc_auc)\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlabel('False Positive Rate')\n","plt.xlabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.legend(loc='lower right')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5s5Vh5CazqFg"},"source":["In questa sezione di codice viene implementata la k-fold cross validation con k fissato a 10. E' importante notare che, a differenza dell'addestramento della rete neurale iniziale, durante la k-fold cross validation, il modello viene riaddestrato da ciascuna iterazione. Tale approccio richiede un maggior tempo di esecuzione rispetto all'addestramento singolo della rete nuerale, ma fornisce una valutazione più affidabile."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":908903,"status":"ok","timestamp":1708748378488,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"-kOEEMR4dFim","outputId":"15101127-043b-4b23-a761-89c3b4030bf0"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense\n","\n","# Fissato il numero di fold e creato un oggetto KFold per la gestione della cross validation\n","n_fold = 10\n","folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n","\n","# Lista per salvare le accuratezze di ogni fold\n","accuracy_k_fold_nn = []\n","\n","for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train, y_train)):\n","    X_train_fold, X_valid_fold = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n","    y_train_fold, y_valid_fold = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n","\n","    # Addestramento del modello\n","    model.fit(X_train_fold, y_train_fold, epochs=40, batch_size=10, verbose=0)\n","\n","    # Otteniamo delle previsioni sul set di validazione\n","    y_valid_pred_prob = model.predict(X_valid_fold)\n","    y_valid_pred = np.round(y_valid_pred_prob)\n","\n","    # Calcolo e salvataggio dell'accuratezza del fold corrente\n","    accuracy_k_fold_nn.append(accuracy_score(y_valid_fold, y_valid_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1708748382945,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"ZsUzYNb9dNy-","outputId":"7f4b052e-b747-40e6-adc3-0d6802402afa"},"outputs":[],"source":["# Stampa delle accuratezze\n","accuracy_k_fold_nn"]},{"cell_type":"markdown","metadata":{"id":"sGG_rf9Uztq3"},"source":["Questo blocco di codice calcola l'intervallo di confidenza al 95% delle accuratezze ottenute durante la k-fold cross validation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":320,"status":"ok","timestamp":1708748387925,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"kvAJHPNbdQaP","outputId":"2002251c-f92b-4b04-ae73-f09ff8de97c3"},"outputs":[],"source":["import scipy.stats as st\n","confidence_interval_nn = st.t.interval(confidence=0.95, df=len(accuracy_k_fold_nn)-1, loc=np.mean(accuracy_k_fold_nn), scale=st.sem(accuracy_k_fold_nn))\n","confidence_interval_nn"]},{"cell_type":"markdown","metadata":{"id":"-14M8NFlzvgn"},"source":["Questa sezione di codice stampa un grafico dell'accuratezza media ottenuta durante la k-fold cross validation, insieme all'intervallo di confidenza al 95%."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":958},"executionInfo":{"elapsed":820,"status":"ok","timestamp":1708748395317,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"uHM52m1RdSad","outputId":"463c52eb-8e51-41f2-b600-2b0fad83d63c"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","\n","# Calcolo della media e dell'intervallo di confidenza su k-fold\n","mean_accuracy_nn = np.mean(accuracy_k_fold_nn)\n","\n","# Stampa grafico della media e dell'intervallo di confidenza\n","plt.errorbar(1, mean_accuracy_nn, yerr=(confidence_interval_nn[1] - confidence_interval_nn[0])/2, fmt='o', label='NN')\n","plt.xlabel('Group')\n","plt.ylabel('Value')\n","plt.title('Mean with Confidence Interval')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ZlPhLoQpeo94"},"source":["# Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"O14Bf4RHhtxw"},"source":["Categorizzazione delle features numeriche"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B6r0PzYXhwxq"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, f1_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","\n","def categorize_bmi(bmi):\n","    if bmi < 19:\n","        return \"Underweight\"\n","    elif 19 <= bmi < 25:\n","        return \"Normal\"\n","    elif 25 <= bmi < 30:\n","        return \"Overweight\"\n","    elif 30 <= bmi < 35:\n","        return \"Obese\"\n","    else:\n","        return \"Extreme Obese\"\n","\n","\n","def categorize_illness_days(days):\n","    if days == 0:\n","        return \"None\"\n","    elif days <= 7:\n","        return \"Low\"\n","    elif days <= 14:\n","        return \"Medium\"\n","    else:\n","        return \"High\"\n","\n","sampled_df['BMI'] = sampled_df['BMI'].apply(categorize_bmi)\n","sampled_df['PhysHlth'] = sampled_df['PhysHlth'].apply(categorize_illness_days)\n","\n","\n","label_encoder = LabelEncoder()\n","sampled_df['PhysHlth'] = label_encoder.fit_transform(sampled_df['PhysHlth'])\n","sampled_df['BMI'] = label_encoder.fit_transform(sampled_df['BMI'])"]},{"cell_type":"markdown","metadata":{"id":"K03QrIqVd5R2"},"source":["Addestramento del modello"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1708748458852,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"FB-EkPr-o_-Z","outputId":"3cd18c95-9f7e-479f-884d-ae1830afc353"},"outputs":[],"source":["from sklearn.naive_bayes import CategoricalNB\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","import time\n","\n","# Split the dataset into features (X) and target variable (y)\n","X = sampled_df.drop(columns=['Diabetes'])\n","y = sampled_df['Diabetes']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Initialize the Categorical Naive Bayes classifier\n","cnb = CategoricalNB()\n","\n","# Measure the start time\n","start_time = time.time()\n","\n","# Train the Categorical Naive Bayes classifier\n","cnb.fit(X_train, y_train)\n","\n","# Measure the end time\n","end_time = time.time()\n","\n","# Calculate the training time\n","nb_training_time = end_time - start_time\n","\n","# Evaluate the model on training data\n","y_train_pred = cnb.predict(X_train)\n","train_accuracy = accuracy_score(y_train, y_train_pred)\n","train_classification_report = classification_report(y_train, y_train_pred)\n","\n","# Evaluate the model on testing data\n","y_prob = cnb.predict(X_test)\n","test_accuracy = accuracy_score(y_test, y_prob)\n","test_classification_report = classification_report(y_test, y_prob)\n","\n","# Print training results\n","print(\"Training Results:\")\n","print(f\"Training Time Naive Bayes: {nb_training_time:.3f} seconds\")\n","print(f\"Training Accuracy: {train_accuracy:.4f}\")\n","print(\"Classification Report on Training Data:\\n\", train_classification_report)\n","\n","# Print testing results\n","print(\"\\nTesting Results:\")\n","print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n","print(\"Classification Report on Testing Data:\\n\", test_classification_report)\n"]},{"cell_type":"markdown","metadata":{"id":"seEKO7m_d-ZL"},"source":["Matrice di confusione"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":935},"executionInfo":{"elapsed":827,"status":"ok","timestamp":1708748471125,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"UTNwm7fMqU-n","outputId":"4f02aaf0-f020-454c-ce2e-1ce779692339"},"outputs":[],"source":["cm = confusion_matrix(y_test,y_prob)\n","labels = [1,0]\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","disp.plot();"]},{"cell_type":"markdown","metadata":{"id":"6spYzUbneBcn"},"source":["Presentazione Curva ROC"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":958},"executionInfo":{"elapsed":991,"status":"ok","timestamp":1708748475463,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"x4-oTsCWq5La","outputId":"59c743de-e6d2-4ac0-c239-11a623be932f"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, f1_score\n","\n","y_probs = cnb.predict_proba(X_test)[:, 1]\n","fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n","roc_auc = auc(fpr, tpr)\n","plt.figure()\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC)')\n","plt.legend(loc=\"lower right\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"e1bBVqcoeM8B"},"source":["10-Fold Cross Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"haltV8Bdq-N5"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score\n","\n","n_fold = 10\n","folds = KFold(n_splits=n_fold, shuffle=True)\n","\n","accuracy_k_fold_nb = []\n","\n","for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train, y_train)):\n","    X_train_fold, X_valid_fold = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n","    y_train_fold, y_valid_fold = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n","\n","    cnb = CategoricalNB()\n","    cnb.fit(X_train_fold, y_train_fold)\n","    # Fai previsioni sul set di validazione e calcola l'accuratezza\n","    y_valid_pred = cnb.predict(X_valid_fold)\n","    accuracy_k_fold_nb.append(accuracy_score(y_valid_fold, y_valid_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1708748481325,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"Ddis2myOO6NG","outputId":"5c9f3675-ce67-4aba-af2b-e86851e9eb68"},"outputs":[],"source":["accuracy_k_fold_nb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1708748483379,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"qblIkBBAQtTG","outputId":"c892018a-b9fe-4e54-e1d7-ecdf9f71bb89"},"outputs":[],"source":["import numpy as np\n","import scipy.stats as st\n","confidence_interval_nb = st.t.interval(confidence=0.90, df=len(accuracy_k_fold_nb)-1, loc=np.mean(accuracy_k_fold_nb), scale=st.sem(accuracy_k_fold_nb))\n","confidence_interval_nb"]},{"cell_type":"markdown","metadata":{"id":"y2nWh4giePUo"},"source":["Intervallo di confidenza per la metrica di accuratezza"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":958},"executionInfo":{"elapsed":682,"status":"ok","timestamp":1708748486452,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"Ge149Ie6rFtU","outputId":"e2eea949-dd2a-43a1-c14d-ae5a85f3b743"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","# Calculate mean and confidence interval on k-fold\n","mean_accuracy_nb = np.mean(accuracy_k_fold_nb)\n","# Plot the mean and confidence interval\n","plt.errorbar(1, mean_accuracy_nb, yerr=(confidence_interval_nb[1] - confidence_interval_nb[0])/2, fmt='o', label='Naive Bayes')\n","# Add labels and title\n","plt.xlabel('Group')\n","plt.ylabel('Value')\n","plt.title('Mean with Confidence Interval')\n","# Show the plot\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BW8JpcXEnQv6"},"source":["# Conclusioni\n"]},{"cell_type":"markdown","metadata":{"id":"j-MCYKARnVG5"},"source":["## Confronto Curve ROC"]},{"cell_type":"markdown","metadata":{"id":"iMQU-gt6wofC"},"source":["Questo codice è finalizzato al confronto delle curve ROC dei tre modelli: Decision Tree, Reti Neurali e Naive Bayes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":727},"executionInfo":{"elapsed":1033,"status":"ok","timestamp":1708748717481,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"B-pZ9ikunkX2","outputId":"56c73b83-a23a-489c-c60c-4b4cd98c13e9"},"outputs":[],"source":["from sklearn.metrics import roc_curve, roc_auc_score\n","import matplotlib.pyplot as plt\n","\n","# Calcolo della curva ROC per ogni modello\n","fpr1, tpr1, thresholds1 = roc_curve(y_test, y_probs)\n","roc_auc1 = roc_auc_score(y_test, y_probs)\n","\n","fpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred_test_bin)\n","roc_auc2 = roc_auc_score(y_test, y_pred_test_bin)\n","\n","fpr3, tpr3, thresholds3 = roc_curve(y_test, y_pred_prob)\n","roc_auc3 = roc_auc_score(y_test, y_pred_prob)\n","\n","# Plot delle curve ROC\n","plt.figure(figsize=(10, 8))\n","\n","# Modello 1 - Naive Bayes\n","plt.plot(fpr1, tpr1, color='darkorange', lw=2, label='Naive Bayes(AUC = %0.2f)' % roc_auc1)\n","\n","# Modello 2 - Rete Neurale\n","plt.plot(fpr2, tpr2, color='blue', lw=2, label='NN(AUC = %0.2f)' % roc_auc2)\n","\n","# Modello 3 - Decision Tree\n","plt.plot(fpr3, tpr3, color='green', lw=2, label='Decision Tree (AUC = %0.2f)' % roc_auc3)\n","\n","# Linea di riferimento\n","plt.plot([0, 1], [0, 1], 'k--', lw=2)\n","\n","# Impostazioni del grafico\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Confronto delle curve ROC')\n","plt.legend(loc=\"lower right\")\n","\n","# Mostra il grafico\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"F6kfDvPtprQE"},"source":["# Confronto Intervalli di confidenza"]},{"cell_type":"markdown","metadata":{"id":"sTM3y1ZUyDCi"},"source":["Questo blocco di codice crea un grafico che rappresenta la media con intervallo di confidenza per i tre modelli: Decision Tree, Reti Neurali e Naive Bayes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":958},"executionInfo":{"elapsed":1764,"status":"ok","timestamp":1708748505647,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"EyCNnFl4pvoj","outputId":"8fdfeede-b540-442a-fc3d-0990997780f4"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","\n","# Calcolo della media e dell'intervallo di confidenza per ogni modello\n","plt.errorbar(1, mean_accuracy_nn, yerr=(confidence_interval_nn[1] - confidence_interval_nn[0])/2, fmt='o', label='NN', color='blue')\n","plt.errorbar(2, mean_accuracy_dt, yerr=(confidence_interval_dt[1] - confidence_interval_dt[0])/2, fmt='o', label='Decision Tree', color='green')\n","plt.errorbar(3, mean_accuracy_nb, yerr=(confidence_interval_nb[1] - confidence_interval_nb[0])/2, fmt='o', label='Naive Bayes', color='orange')\n","\n","# Stampa il grafico\n","plt.xlabel('Group')\n","plt.ylabel('Value')\n","plt.title('Mean with Confidence Interval')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"Up1WIVhs7I-o"},"source":["# Tempi di esecuzione\n"]},{"cell_type":"markdown","metadata":{"id":"nyrRuloAyUp9"},"source":["Questo codice stampa i tempi di training dei tre modelli: Decision Tree, Reti Neurali e Naive Bayes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":277,"status":"ok","timestamp":1708748509082,"user":{"displayName":"Damiano Ficara","userId":"03515831605778384878"},"user_tz":-60},"id":"2ATouHUx7NsN","outputId":"9d826009-3fef-43c4-f513-1916229a0209"},"outputs":[],"source":["print(f\"Tempo di training Decision tree: {dt_training_time:.3f} secondi\")\n","print(f\"Tempo di training Neural Network: {nn_training_time:.3f} secondi\")\n","print(f\"Tempo di training Naive Bayes: {nb_training_time:.3f} secondi\")"]}],"metadata":{"colab":{"collapsed_sections":["IHGuzgm6295q","JC9FKrbXvuYp","YSA52dxMXdaS","bnsv3VuebNBV","yCZVROPoH3x8","asmYiy7OBrZ8","AODszHhsl4OW","qAe66uneDoqD","91kMcG-sKUq9","ZlPhLoQpeo94","j-MCYKARnVG5","F6kfDvPtprQE"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
