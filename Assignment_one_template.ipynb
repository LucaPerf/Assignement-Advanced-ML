{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH5ja_uiJbr6"
      },
      "source": [
        "# Predicting Default Payments with Fully-Connected NNs\n",
        "\n",
        "The dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Description\n",
        "This dataset employs a binary variable to indicate whether a credit card payment occurred (1 = Yes, 0 = No). The study selected the following 23 factors as explanatory variables:\n",
        "\n",
        "- Variable 1: Amount of credit granted (in local currency), which includes both individual credit and family (supplementary) credit.\n",
        "- Variable 2: Gender (1 = male; 2 = female).\n",
        "- Variable 3: Education level (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
        "- Variable 4: Age (years).\n",
        "- Variables 5-10: Payment history over several months. The scale for payment status ranges from -1 (paid on time) to 9 (delayed by nine months or more). It tracks payments from April to September:\n",
        "\n",
        "    - Variable 5: Payment status in September;\n",
        "    - Variable 6: Payment status in August;\n",
        "    - Variable 7: Payment status in July;\n",
        "    - Variable 8: Payment status in June;\n",
        "    - Variable 9: Payment status in May;\n",
        "    - Variable 10: Payment status in April. \n",
        "- Variables 11-16: Amount of monthly billing (in local currency), tracking statements from September to April.\n",
        "- Variables 17-22: Amount of previous payments (in local currency), corresponding to monthly payments made from September to April."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-FgzT_cJbsH"
      },
      "source": [
        "## Inspecting the data\n",
        "\n",
        "any comment about data dimensionality/distribution goes here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import librerie\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vSiz47HXYYM"
      },
      "outputs": [],
      "source": [
        "# carichiamo il dataset\n",
        "train_data = pd.read_csv(\"train.csv\")\n",
        "test_data = pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mostra prime righe del dataset train\n",
        "df_train_data = train_data.head() \n",
        "print(df_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Controlla la dimensionalità del dataset di training e test\n",
        "print(\"Dimensionalità del dataset di training:\", train_data.shape)\n",
        "print(\"Dimensionalità del dataset di test:\", test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train_data.info() # otteniamo informazioni sulle colonne\n",
        "\n",
        "missing_values = df_train_data.isnull().sum()\n",
        "print(missing_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# otteniamo statistiche descrittive per le variabili numeriche \n",
        "print(df_train_data.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# controllo presenza di valori nulli\n",
        "df_train_data.isnull().sum().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# controllo di presenza di valori duplicati\n",
        "df_train_data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analisi statica univariata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "column = 'default payment next month'\n",
        "total_rows = len(df_train_data)\n",
        "counts = df_train_data[column].value_counts()\n",
        "percentages = [count / total_rows * 100 for count in counts]\n",
        "plt.pie(percentages, autopct='%1.1f%%', colors=['green', 'orange'])\n",
        "plt.title(f'Proportion of {column} (target)')\n",
        "labels = ['yes', 'no']\n",
        "plt.legend(labels=labels, loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analisi della variabile target\n",
        "# Conta la distribuzione della variabile target\n",
        "sns.countplot(x='default payment next month', data=df_train_data)\n",
        "plt.title('Distribuzione di default payment next month')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# distribuzione delle feature numeriche \n",
        "\n",
        "numerical_columns = ['LIMIT_BAL', 'AGE', \n",
        "                     'PAY_AMT1', 'PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6', \n",
        "                     'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\n",
        "\n",
        "# Istogramma per ogni variabile numerica\n",
        "bins = np.arange(df_train_data[column].min(), df_train_data[column].max() + 2)\n",
        "df_train_data[numerical_columns].hist(figsize=(16, 12), bins=bins, color='green')\n",
        "\n",
        "#df_train_data[numerical_columns].hist(figsize=(16, 12), bins=20, color='green')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# distribuzione feature categoriche \n",
        "categorical_columns = ['SEX', \n",
        "                       'PAY_0', 'PAY_2','PAY_3','PAY_4',\n",
        "                       'EDUCATION', 'MARRIAGE']\n",
        "\n",
        "def print_categoric_feature(column):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    #sns.countplot(data=df_train_data, x=column, color='orange', legend=True)\n",
        "    sns.countplot(data=df_train_data, x=column, hue=column, palette='Set2', legend=False)\n",
        "    plt.title(f'{column}')\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "for i in categorical_columns:\n",
        "    print_categoric_feature(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcola la correlazione tra ogni feature e il target\n",
        "correlation = df_train_data.drop('default payment next month', axis=1).corrwith(df_train_data['default payment next month'])\n",
        "\n",
        "# Crea un grafico a barre per visualizzare le correlazioni\n",
        "plt.figure(figsize=(12, 8))\n",
        "correlation.plot(kind='bar', grid=True, color='orange')\n",
        "plt.title(\"Correlazione con 'default payment next month'\")\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Correlazione\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# relazioni tra variabili\n",
        "# Mappa di correlazione\n",
        "plt.figure(figsize=(16, 13))\n",
        "correlation_matrix = df_train_data.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Matrice di correlazione delle variabili numeriche')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjWrQr5vWTTG"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "describe the choice made during the preprocessing operations, also taking into account the previous considerations during the data inspection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J84aUJVUJbsI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb9aljYxJbsK"
      },
      "source": [
        "## Building the network\n",
        "\n",
        "any description/comment about the procedure you followed in the choice of the network structure and hyperparameters goes here, together with consideration about the training/optimization procedure (e.g. optimizer choice, final activations, loss functions, training metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Con il dataset grezzo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS3zsRlBYHKW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Input\n",
        "from keras.optimizers import SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estrazione delle features e variabile target\n",
        "y = df_train_data['default payment next month']\n",
        "X = df_train_data.drop(columns=['default payment next month'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Suddivisione dataset in training set e test set (con dimensione del test_size del 30%)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "y_train = to_categorical(y_train, num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_classes = y_train.shape[0]\n",
        "print(nb_classes, 'classes')\n",
        "\n",
        "dims = X_train.shape[1]\n",
        "print(X_train.shape, 'dims Training set')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input((dims,)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer1 = SGD(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer1, loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history1 = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_loss(history):\n",
        "  x_plot = list(range(1,len(history.history[\"loss\"])+1))\n",
        "  plt.figure()\n",
        "  plt.title(\"Loss\")\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(x_plot, history.history['loss'])\n",
        "  plt.plot(x_plot, history.history['val_loss'])\n",
        "  plt.legend(['Training', 'Validation'])\n",
        "\n",
        "def plot_accuracy(history):\n",
        "  x_plot = list(range(1,len(history.history[\"accuracy\"])+1))\n",
        "  plt.figure()\n",
        "  plt.title(\"Accuracy\")\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.plot(x_plot, history.history['accuracy'])\n",
        "  plt.plot(x_plot, history.history['val_accuracy'])\n",
        "  plt.legend(['Training', 'Validation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_loss(history1)\n",
        "plot_accuracy(history1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### inizio modifiche"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Supponiamo che il tuo dataset sia un DataFrame chiamato df\n",
        "# df = pd.read_csv('tuo_dataset.csv')  # Carica i dati\n",
        "\n",
        "# Seleziona le colonne di input e il target\n",
        "X = df_train_data.drop('default payment next month', axis=1)  # Rimuovi la colonna target\n",
        "y = df_train_data['default payment next month']  # Colonna target\n",
        "\n",
        "# Splitta il dataset in training e test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalizza i dati (se necessario)\n",
        "x_train = x_train.astype('float32') / x_train.max()  # Normalizza i dati a [0, 1]\n",
        "x_test = x_test.astype('float32') / x_test.max()\n",
        "\n",
        "# Se 'y_train' è binaria (0 o 1), non hai bisogno di to_categorical.\n",
        "# Se hai più classi, utilizza to_categorical.\n",
        "y_train = to_categorical(y_train)  # Solo se hai più classi (0, 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dims = x_train.shape[1]\n",
        "print('Input Shape =', dims)\n",
        "\n",
        "nb_classes = y_train.shape[1]\n",
        "print('Number classes = Output Shape =', nb_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input((dims,)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compile the model\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predict\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "int_predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(int_predictions[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "iperparametri da vedere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# build the network \n",
        "dims = x_train.shape[1]\n",
        "print('Input Shape =', dims)\n",
        "\n",
        "nb_classes = y_train.shape[1]\n",
        "print('Number classes = Output Shape =', nb_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input((dims,)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(2, activation='relu'))\n",
        "model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "optimizer = SGD(learning_rate=0.001)\n",
        "\n",
        "#model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
        "#              metrics=['accuracy'])\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# training\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=50, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss(history):\n",
        "  x_plot = list(range(1,len(history.history[\"loss\"])+1))\n",
        "  plt.figure()\n",
        "  plt.title(\"Loss\")\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(x_plot, history.history['loss'])\n",
        "  plt.plot(x_plot, history.history['val_loss'])\n",
        "  plt.legend(['Training', 'Validation'])\n",
        "\n",
        "def plot_accuracy(history):\n",
        "  x_plot = list(range(1,len(history.history[\"accuracy\"])+1))\n",
        "  plt.figure()\n",
        "  plt.title(\"Accuracy\")\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.plot(x_plot, history.history['accuracy'])\n",
        "  plt.plot(x_plot, history.history['val_accuracy'])\n",
        "  plt.legend(['Training', 'Validation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_loss(history)\n",
        "plot_accuracy(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w7UIdtIWuCD"
      },
      "source": [
        "## Analyze and comment the training results\n",
        "\n",
        "here goes any comment/visualization of the training history and any initial consideration on the training results  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4brGmh1BJbsL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jiOZzvyJbsN"
      },
      "source": [
        "## Validate the model and comment the results\n",
        "\n",
        "please describe the evaluation procedure on a validation set, commenting the generalization capability of your model (e.g. under/overfitting). You may also describe the performance metrics that you choose: what is the most suitable performance measure (or set of performance measures) in this case/dataset, according to you? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgGlAIaEJbsO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MhCwXroWmf9"
      },
      "source": [
        "## Make predictions (on the provided test set)\n",
        "\n",
        "Based on the results obtained and analyzed during the training and the validation phases, what are your (rather _personal_) expectations with respect to the performances of your model on the blind external test set? Briefly motivate your answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbtA2vJRWpMY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w-sa4AlaBJg"
      },
      "source": [
        "# OPTIONAL -- Export the predictions in the format indicated in the assignment release page and verify you prediction on the [assessment page](https://aml-assignmentone-2425.streamlit.app/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTPSYsbVaAQ_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
