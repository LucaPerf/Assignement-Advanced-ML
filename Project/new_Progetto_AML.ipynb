{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications import ResNet101, VGG19\n",
    "import matplotlib.image as mpimg\n",
    "from anytree import Node, RenderTree\n",
    "from anytree.exporter import DotExporter\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"C:\\Users\\lperf\\OneDrive\\Desktop\\Magistrale\\2° Anno\\Advanced Machine Learning\\AdvancedML - Laboratory\\Assignement-Advanced-ML\\Project\\TrainingFabio_Extracted_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries and lists\n",
    "labels_2_int = {}\n",
    "labels_1_array = []\n",
    "\n",
    "# Iterate over the subdirectories in the order they appear\n",
    "dataset_dir = os.path.join(dataset_path, \"train\")\n",
    "for i, folder_name in enumerate(sorted(os.listdir(dataset_dir))):\n",
    "    labels_2_int[folder_name] = i\n",
    "    print(folder_name)\n",
    "\n",
    "    first_part = folder_name.split(' ')[0]\n",
    "    if first_part not in labels_1_array:  # Keeps the first occurrence\n",
    "        labels_1_array.append(first_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_2_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1_int = {}\n",
    "for i, fruit_name in enumerate(labels_1_array):\n",
    "    # Map folder name to an integer\n",
    "    labels_1_int[fruit_name] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save(input_dir, output_file, show_images=False):\n",
    "    data = []\n",
    "    # categoria generale (es mela)\n",
    "    labels_1 = []\n",
    "    # categoria specifica (es mela golden)\n",
    "    labels_2 = []\n",
    "\n",
    "    # support variables for plotting\n",
    "    already_printed = []\n",
    "    images_to_plot = []\n",
    "    labels_to_plot = []\n",
    "\n",
    "    class_names = sorted(os.listdir(input_dir))  # Assicura ordine costante delle classi\n",
    "    for _, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(input_dir, class_name)\n",
    "\n",
    "        # Use os.path.split to correctly handle path separators in a platform-independent way\n",
    "        _, folder_name = os.path.split(class_dir)\n",
    "\n",
    "        label_1 = labels_1_int[folder_name.split(' ')[0]]  # Access label_1\n",
    "        label_2 = labels_2_int[folder_name]              # Access label_2\n",
    "\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            img = load_img(img_path, target_size=(128, 128))  # Ridimensiona\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = preprocess_input(img_array)  # Normalizzazione per VGG19\n",
    "            data.append(img_array)\n",
    "            labels_1.append(label_1)\n",
    "            labels_2.append(label_2)\n",
    "\n",
    "            # Store image and label for plotting\n",
    "            if label_2 not in already_printed:\n",
    "                images_to_plot.append(img)\n",
    "                labels_to_plot.append(folder_name)  # Use folder_name directly\n",
    "                already_printed.append(label_2)\n",
    "\n",
    "    if show_images:\n",
    "        # Plot images and labels\n",
    "        num_images = len(images_to_plot)\n",
    "        num_cols = 10\n",
    "        num_rows = (num_images + num_cols - 1) // num_cols\n",
    "\n",
    "        plt.figure(figsize=(20, num_rows * 2))\n",
    "        for i in range(num_images):\n",
    "            plt.subplot(num_rows, num_cols, i + 1)\n",
    "            plt.imshow(images_to_plot[i])\n",
    "            plt.title(labels_to_plot[i])\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "        #save image to file\n",
    "        plt.savefig('plot.png')\n",
    "\n",
    "    # Converti in array numpy\n",
    "    data = np.array(data)\n",
    "    labels_1 = np.array(labels_1)\n",
    "    labels_2 = np.array(labels_2)\n",
    "\n",
    "    # Controllo se è train o validation per applicare lo shuffle\n",
    "    if 'train' in input_dir:\n",
    "        indices = np.random.permutation(len(data))  # Genera indici casuali\n",
    "        data = data[indices]\n",
    "        labels_1 = labels_1[indices]\n",
    "        labels_2 = labels_2[indices]\n",
    "        print(\"shuffle eseguito\")\n",
    "    else:\n",
    "        print(\"shuffle solo per il train\")\n",
    "\n",
    "    # Salva i dati\n",
    "    np.savez(output_file, x=data, y1=labels_1, y2=labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_path = dataset_path + 'train_data_BIG.npz'\n",
    "train_path = 'train_data_BIG.npz'\n",
    "# check if npz file already exists\n",
    "if not os.path.exists(train_path):\n",
    "    preprocess_and_save(dataset_path+'\\\\train', train_path, show_images=True)\n",
    "else:\n",
    "    image_path = r\"C:\\Users\\lperf\\OneDrive\\Desktop\\Magistrale\\2° Anno\\Advanced Machine Learning\\Assignement\\Project\\TrainingFabio_Extracted_data\"\n",
    "    # plot local image using plt\n",
    "    img = mpimg.imread(image_path + '\\\\plot.png')\n",
    "    # Create a figure with a large size\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))  # Adjust size as needed\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Maximize the axis size by turning off the spines and ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # Show the image\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = 'val_data_BIG.npz'\n",
    "# check if npz file already exists\n",
    "if not os.path.exists(val_path):\n",
    "    preprocess_and_save(dataset_path + '\\\\val', val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path ='test_data_BIG.npz'\n",
    "# check if npz file already exists\n",
    "if not os.path.exists(test_path):\n",
    "    preprocess_and_save(dataset_path+'\\\\test', test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"C:\\Users\\lperf\\OneDrive\\Desktop\\Magistrale\\2° Anno\\Advanced Machine Learning\\AdvancedML - Laboratory\\Assignement-Advanced-ML\\Project\\train_data_BIG.npz\"\n",
    "test_path = r\"C:\\Users\\lperf\\OneDrive\\Desktop\\Magistrale\\2° Anno\\Advanced Machine Learning\\AdvancedML - Laboratory\\Assignement-Advanced-ML\\Project\\test_data_BIG.npz\"\n",
    "val_path = r\"C:\\Users\\lperf\\OneDrive\\Desktop\\Magistrale\\2° Anno\\Advanced Machine Learning\\AdvancedML - Laboratory\\Assignement-Advanced-ML\\Project\\val_data_BIG.npz\"\n",
    "\n",
    "# load train data\n",
    "data_train = np.load(train_path)\n",
    "x_train, y1_train, y2_train = data_train['x'], data_train['y1'], data_train['y2']\n",
    "\n",
    "# Load validation data\n",
    "data_val = np.load(val_path)\n",
    "x_val, y1_val, y2_val = data_val['x'], data_val['y1'], data_val['y2']\n",
    "\n",
    "# Load test data\n",
    "data_test = np.load(test_path)\n",
    "x_test, y1_test, y2_test = data_test['x'], data_test['y1'], data_test['y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_train = to_categorical(y1_train, num_classes=70)\n",
    "y1_val = to_categorical(y1_val, num_classes=70)\n",
    "y1_test = to_categorical(y1_test, num_classes=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_train = to_categorical(y2_train, num_classes=123)\n",
    "y2_val = to_categorical(y2_val, num_classes=123)\n",
    "y2_test = to_categorical(y2_test, num_classes=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_object_label_1 = y1_train.shape[1]\n",
    "num_object_label_2 = y2_train.shape[1]\n",
    "print(f\"Numero di classi per label_1: {num_object_label_1}\")\n",
    "print(f\"Numero di classi per label_2: {num_object_label_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contiamo il numero di occorrenze per ogni etichetta secondaria\n",
    "y2_counts = np.sum(y2_train, axis=0)  # Conta le occorrenze di ogni classe secondaria\n",
    "\n",
    "# Creiamo una struttura ad albero\n",
    "root = Node(\"Frutti\")  # Nodo radice\n",
    "primary_nodes = {}\n",
    "\n",
    "# Aggiungi nodi per le label primarie\n",
    "for primary_label in labels_1_int:\n",
    "    primary_nodes[primary_label] = Node(f\"{primary_label} (0)\", parent=root)\n",
    "\n",
    "# Aggiungi nodi per le label secondarie con conteggio\n",
    "for class_name, index in labels_2_int.items():\n",
    "    primary_label = class_name.split(' ')[0]  # Estrarre la label primaria\n",
    "    count = int(y2_counts[index])  # Numero di occorrenze per questa classe\n",
    "    if primary_label in primary_nodes:\n",
    "        Node(f\"{class_name} ({count})\", parent=primary_nodes[primary_label])\n",
    "\n",
    "# Aggiorna i conteggi delle label primarie\n",
    "for primary_label, node in primary_nodes.items():\n",
    "    total_count = sum(\n",
    "        int(child.name.split(\"(\")[-1].strip(\")\")) for child in node.children\n",
    "    )\n",
    "    node.name = f\"{primary_label} ({total_count})\"\n",
    "\n",
    "# Stampa l'albero con conteggi\n",
    "for pre, _, node in RenderTree(root):\n",
    "    print(f\"{pre}{node.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def plot_label_distribution(y_train, label_names, title=\"Distribuzione delle etichette\"):\n",
    "    \"\"\"\n",
    "    Visualizza la distribuzione delle etichette in un dataset.\n",
    "\n",
    "    :param y_train: array numpy (one-hot encoded)\n",
    "    :param label_names: lista dei nomi delle classi\n",
    "    :param title: titolo del grafico\n",
    "    \"\"\"\n",
    "    # Converti one-hot encoding in indici delle classi\n",
    "    class_indices = np.argmax(y_train, axis=1)  # Trova la classe con valore massimo\n",
    "\n",
    "    # Conta la frequenza di ogni classe\n",
    "    label_counts = Counter(class_indices)\n",
    "\n",
    "    # Estrai etichette e frequenze\n",
    "    labels = [label_names[idx] for idx in label_counts.keys()]\n",
    "    values = list(label_counts.values())\n",
    "\n",
    "    # Imposta la figura con due sottotrame\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "    # Diagramma a torta\n",
    "    axes[0].pie(values, labels=labels, startangle=90, colors=plt.cm.tab20.colors, autopct='%1.1f%%')\n",
    "    axes[0].set_title(f\"{title} (Pie Chart)\")\n",
    "    axes[0].axis('equal')  # Assicura che il diagramma sia un cerchio\n",
    "\n",
    "    # Diagramma a barre\n",
    "    axes[1].bar(labels, values, color=plt.cm.tab20.colors[:len(labels)])\n",
    "    axes[1].set_title(f\"{title} (Bar Chart)\")\n",
    "    axes[1].set_xlabel(\"Etichette\")\n",
    "    axes[1].set_ylabel(\"Frequenza\")\n",
    "    axes[1].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)  # Rimuove le etichette\n",
    "\n",
    "    # Mostra il grafico\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribuzione di y2_train\n",
    "label_names_y2 = list(labels_2_int.keys())  # Nomi per y2\n",
    "plot_label_distribution(y2_train, label_names_y2, title=\"Distribuzione di Y2 (Sottoclassi)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converti one-hot encoding in indici delle classi\n",
    "class_indices = np.argmax(y2_train, axis=1)\n",
    "\n",
    "# Conta le occorrenze di ogni classe\n",
    "class_counts = Counter(class_indices)\n",
    "\n",
    "# Filtra le classi con meno di 250 e più di 350 immagini\n",
    "class_over_350 = {label_names_y2[idx]: count for idx, count in class_counts.items() if count > 350}\n",
    "class_under_250 = {label_names_y2[idx]: count for idx, count in class_counts.items() if count < 250}\n",
    "\n",
    "# Imposta la figura con due subplot affiancati\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# Grafico per le classi con più di 350 immagini\n",
    "axes[0].bar(class_over_350.keys(), class_over_350.values(), color='green')\n",
    "axes[0].set_xlabel('Classi')\n",
    "axes[0].set_ylabel('Numero di immagini')\n",
    "axes[0].set_title('Classi con più di 350 immagini')\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Grafico per le classi con meno di 250 immagini\n",
    "axes[1].bar(class_under_250.keys(), class_under_250.values(), color='red')\n",
    "axes[1].set_xlabel('Classi')\n",
    "axes[1].set_title('Classi con meno di 250 immagini')\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Mostra il grafico\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribuzione di y1_train\n",
    "label_names_y1 = list(labels_1_int.keys())  # Nomi per y1\n",
    "plot_label_distribution(y1_train, label_names_y1, title=\"Distribuzione di Y1 (Classi Principali)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y_train):\n",
    "    # Converti one-hot encoding in indici delle classi\n",
    "    class_indices = np.argmax(y_train, axis=1)\n",
    "\n",
    "    # Conta le occorrenze di ogni classe\n",
    "    class_counts = Counter(class_indices)\n",
    "\n",
    "    # Numero totale di sample e di classi\n",
    "    total_samples = len(class_indices)\n",
    "    num_classes = len(class_counts)\n",
    "\n",
    "    # Calcola i pesi: più rara è la classe, più alto è il peso\n",
    "    class_weights = {cls: total_samples / (num_classes * count) for cls, count in class_counts.items()}\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola i pesi per y2\n",
    "class_weights_y2 = compute_class_weights(y2_train)\n",
    "\n",
    "# Mostra i pesi calcolati\n",
    "print(\"Pesi per y2_train:\", class_weights_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola i pesi per y1\n",
    "class_weights_y1 = compute_class_weights(y1_train)\n",
    "\n",
    "# Mostra i pesi calcolati\n",
    "print(\"Pesi per y1_train:\", class_weights_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sample_weights(y_train, class_weights):\n",
    "    # Converti da one-hot a indici delle classi\n",
    "    class_indices = np.argmax(y_train, axis=1)\n",
    "\n",
    "    # Assegna il peso corrispondente a ciascun sample\n",
    "    sample_weights = np.array([class_weights[idx] for idx in class_indices])\n",
    "\n",
    "    return sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera i pesi per ogni sample\n",
    "sample_weights_y2 =  np.array(compute_sample_weights(y2_train, class_weights_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera i pesi per ogni sample\n",
    "sample_weights_y1 = np.array( compute_sample_weights(y1_train, class_weights_y1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine sample weights for y1 and y2\n",
    "combined_sample_weights = (sample_weights_y1 + sample_weights_y2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_conformity(x, y1, y2, sample_weights_y1=None, sample_weights_y2=None, dataset_name=\"train\"):\n",
    "    errors = []\n",
    "\n",
    "    # 1 Controllo dimensione dataset\n",
    "    num_samples = x.shape[0]\n",
    "\n",
    "    if y1.shape[0] != num_samples:\n",
    "        errors.append(f\"❌ {dataset_name}: y1 ha {y1.shape[0]} campioni, ma x ne ha {num_samples}\")\n",
    "\n",
    "    if y2.shape[0] != num_samples:\n",
    "        errors.append(f\"❌ {dataset_name}: y2 ha {y2.shape[0]} campioni, ma x ne ha {num_samples}\")\n",
    "\n",
    "    # 2️ Controllo se le etichette sono one-hot encoded\n",
    "    if len(y1.shape) != 2:\n",
    "        errors.append(f\"❌ {dataset_name}: y1 dovrebbe essere one-hot encoded (shape 2D), ma ha shape {y1.shape}\")\n",
    "\n",
    "    if len(y2.shape) != 2:\n",
    "        errors.append(f\"❌ {dataset_name}: y2 dovrebbe essere one-hot encoded (shape 2D), ma ha shape {y2.shape}\")\n",
    "\n",
    "    # 3️ Controllo sample_weights per y1\n",
    "    if sample_weights_y1 is not None:\n",
    "        if sample_weights_y1.shape[0] != num_samples:\n",
    "            errors.append(f\"❌ {dataset_name}: sample_weights_y1 ha {sample_weights_y1.shape[0]} campioni, ma x ne ha {num_samples}\")\n",
    "\n",
    "    # 4️ Controllo sample_weights per y2\n",
    "    if sample_weights_y2 is not None:\n",
    "        if sample_weights_y2.shape[0] != num_samples:\n",
    "            errors.append(f\"❌ {dataset_name}: sample_weights_y2 ha {sample_weights_y2.shape[0]} campioni, ma x ne ha {num_samples}\")\n",
    "\n",
    "    # Se non ci sono errori, tutto è conforme\n",
    "    if not errors:\n",
    "        print(f\"✅ {dataset_name}: Tutti i dati sono conformi!\")\n",
    "    else:\n",
    "        for error in errors:\n",
    "            print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_conformity(x_train, y1_train, y2_train, sample_weights_y1, sample_weights_y2, dataset_name=\"Train\")\n",
    "check_data_conformity(x_val, y1_val, y2_val, dataset_name=\"Validation\")\n",
    "check_data_conformity(x_test, y1_test, y2_test, dataset_name=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci la CNN con due rami\n",
    "def build_model(input_shape, num_classes, num_object_names):\n",
    "    # Input per l'immagine\n",
    "    img_input = Input(shape=input_shape)\n",
    "\n",
    "    # Struttura condivisa della CNN\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(img_input)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Ramo 1 (Classificazione della classe)\n",
    "    class_output = Dense(num_classes, activation='softmax', name='y1')(x)\n",
    "\n",
    "    # Ramo 2 (Classificazione del nome oggetto)\n",
    "    object_output = Dense(num_object_names, activation='softmax', name='y2')(x)\n",
    "\n",
    "    # Modello finale\n",
    "    model = Model(inputs=img_input, outputs=[class_output, object_output])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "num_classes_1 = len(y1_train[0])\n",
    "num_classes_2 = len(y2_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il modello VGG19 pre-addestrato\n",
    "VGG_model5 = VGG19(\n",
    "                weights='imagenet', \n",
    "                include_top=False, \n",
    "                input_shape=(128, 128, 3),\n",
    "                input_tensor=None,\n",
    "                pooling=None,\n",
    "                classifier_activation=\"softmax\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_model5.trainable = False  # Blocca i pesi del modello pre-addestrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggiungi strati personalizzati\n",
    "x = layers.GlobalAveragePooling2D()(VGG_model5.output)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "# 🔴 Ramo 1 - Predizione del Frutto\n",
    "fruit_output = layers.Dense(num_classes_1, activation=\"softmax\", name=\"y1\")(x)\n",
    "\n",
    "# 🔵 Ramo 2 - Predizione della Qualità\n",
    "quality_output = layers.Dense(num_classes_2, activation=\"softmax\", name=\"y2\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea il modello con doppia uscita\n",
    "model5 = Model(inputs=VGG_model5.input, outputs=[fruit_output, quality_output])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci il numero di classi\n",
    "num_classes_1 = len(y1_train[0])  # Classi dei frutti\n",
    "num_classes_2 = len(y2_train[0])  # Classi della qualità\n",
    "\n",
    "# Compila il modello\n",
    "model5.compile(\n",
    "    optimizer='adam',\n",
    "\n",
    "    loss={\n",
    "        'y1': 'categorical_crossentropy', \n",
    "        'y2': 'categorical_crossentropy'\n",
    "        },\n",
    "\n",
    "    metrics={\n",
    "        'y1': 'accuracy', \n",
    "        'y2': 'accuracy'\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Controlla la perdita sulla validation set\n",
    "    patience=2,          # Numero di epoche senza miglioramento prima di fermarsi\n",
    "    min_delta=0.000001,     # Soglia per considerare un miglioramento\n",
    ")\n",
    "\n",
    "# Addestramento del modello\n",
    "history5 = model5.fit(\n",
    "    x_train,\n",
    "    {'y1': y1_train, \n",
    "     'y2': y2_train},\n",
    "    validation_data=(\n",
    "        x_val, {\n",
    "            'y1': y1_val, \n",
    "            'y2': y2_val}),\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Visualizza i grafici di loss e accuracy per un modello con due output.\n",
    "    \n",
    "    Args:\n",
    "        history: Storia dell'addestramento del modello (output di model.fit())\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 🔴 Grafico della Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], 'r-', label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], 'r--', label='Validation Loss')\n",
    "    plt.plot(epochs, history.history['y1_loss'], 'b-', label='Y1 (Fruit) Loss')\n",
    "    plt.plot(epochs, history.history['val_y1_loss'], 'b--', label='Val Y1 Loss')\n",
    "    plt.plot(epochs, history.history['y2_loss'], 'g-', label='Y2 (Quality) Loss')\n",
    "    plt.plot(epochs, history.history['val_y2_loss'], 'g--', label='Val Y2 Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training & Validation Loss')\n",
    "    \n",
    "    # 🔵 Grafico della Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['y1_accuracy'], 'b-', label='Y1 (Fruit) Accuracy')\n",
    "    plt.plot(epochs, history.history['val_y1_accuracy'], 'b--', label='Val Y1 Accuracy')\n",
    "    plt.plot(epochs, history.history['y2_accuracy'], 'g-', label='Y2 (Quality) Accuracy')\n",
    "    plt.plot(epochs, history.history['val_y2_accuracy'], 'g--', label='Val Y2 Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training & Validation Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dopo l'addestramento del modello\n",
    "plot_training_history(history5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.save(\"modelVGG19Keras5.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
